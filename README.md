# Neural Style Transfer-implementation-Pytorch
A Neural Algorithm of Artistic Style ( a.k.a Neural Style Transfer ) [ CVPR2016 ] 논문을 Pytorch 로 구현합니다.

# 선정 이유
- 데이터셋을 특별히 준비 해 둘 필요가 없다
- 아키텍쳐링이 쉽고 논문 구현 난이도가 높지 않다
- Generative Model로 결과가 시각적으로 볼 수 있다.

# 학습 내용
LBFGS optim 사용 방법 (closure 구현) 학습

Transfer 논문 모델 원리 이해

# 의문점1 ( 고찰 )

내적을 통해 유사도를 파악할 수 있음. 즉 Style 손실을 계산하기 위해 내적을 하는 것은 알수 있음.

그러면 왜 style image , gen image 의 손실을 구할때 Gram Matrix를 활용하는가? 

즉 style image 와 gen image 끼리 내적하고 손실을 구하는게 아니라 (stlye image 내적 stlye image) 와 (gen image 내적 gen image) 끼리 비교하는 것인가?

다시말해, 스타일 변환(Style Transfer)에서 스타일 손실(Style Loss)을 계산할 때 스타일 이미지와 생성된 이미지의 특징 맵을 직접 내적해서 비교하지 않고, 각각의 Gram Matrix를 만들어 비교하는 이유는 무엇인가?

## 1. Gram Matrix가 캡처하는 정보

Gram Matrix **𝐺**는 **특징 맵(Feature Map) 간의 상관관계(Correlation)를 나타내는 행렬**

![image](https://github.com/user-attachments/assets/9a1b1a81-1166-4513-b957-d909b8f948e1)

즉, 특징 맵의 채널(필터) 간 **유사도(Style Information)** 를 저장하는 역할. 스타일이란 개별적인 픽셀의 값이 아니라, 전체적인 패턴과 질감에 대한 정보이기 때문.

## 2. 직접 내적하지 않는 이유

### (1) 스타일은 위치 정보가 중요하지 않음
- 콘텐츠 손실(Content Loss)은 픽셀 단위로 유사성을 측정하지만, 스타일은 픽셀이 어디에 위치하는지보다는 전체적인 질감과 패턴을 유지하는 것이 중요.
- 만약 스타일 이미지와 생성된 이미지의 특징 맵을 직접 비교하면, 스타일이 비슷하지만 위치가 다를 경우에도 큰 손실이 발생할 수 있음.
### (2) Gram Matrix는 공간 정보를 제거함
- 스타일 이미지와 생성된 이미지의 특징 맵을 각각 Gram Matrix로 변환하면, 특징 간의 **공간적 배치 정보(Position Information)** 가 사라지고, 전체적인 통계적 특성만 남게 됨.
- 즉, 스타일의 전체적인 분포가 유지되는지 확인하는 역할을 함.
### (3) 예제: 색깔을 섞어도 같은 스타일로 인식
예를 들어, 스타일 이미지가 붓 터치 질감이 강한 인상파 그림이라고 가정한다면

- 스타일 이미지의 붓 터치가 어디에 있느냐보다는 전체적으로 얼마나 강하게, 어떤 패턴으로 분포하는지가 중요.
- 생성된 이미지도 같은 붓 터치 패턴을 가지면, 위치가 다르더라도 같은 스타일로 간주해야 함.
- 이를 위해, 특징 맵의 Gram Matrix를 비교하여 스타일을 학습하는 것.

## 3. 정리
- 스타일은 "특징의 상대적인 강도"를 표현하는 것이지, "특징이 어디에 있는가"가 중요하지 않음.
- Gram Matrix를 사용하면 공간적 배치를 무시하고 스타일의 통계적 특성을 비교할 수 있음.
- 스타일 이미지와 생성된 이미지를 직접 비교하는 것이 아니라, 각각의 Gram Matrix를 비교해야 진짜 스타일을 유지할 수 있음.


# 의문점2 ( 고찰 )

content image도 내적으로 유사도 차이를 했다면 어땠을까? 

Gram Matrix 기반 비교(스타일 비교)가아니라 content 이미지와 gen 이미지의 직접 내적을 사용하여 유사도를 파악하는 것은 어떨까?

즉, **"Gram Matrix가 아니라 콘텐츠 이미지와 생성된 이미지를 직접 내적해서 유사도를 비교하면 어떨까?"**

## 1. 현재 Content Loss 방식 (픽셀 단위 차이 비교)

Style Transfer에서 콘텐츠 손실(Content Loss)은 보통 다음처럼 정의:
![image](https://github.com/user-attachments/assets/30f389a1-9b26-48ad-a6f1-1e597769c6f3)

F content : 콘텐츠 이미지의 특징 맵

F gen : 생성된 이미지의 특징 맵

같은 위치의 특징 벡터를 직접 비교해서 차이를 최소화하는 방식.

## 2. 콘텐츠 이미지와 생성된 이미지를 직접 내적

손실 비교를 다음과 같이 변경한다면

![image](https://github.com/user-attachments/assets/7d3d800e-7a0a-4b16-bd9d-96fd499de85a)

- 여기서 **내적(Dot Product)** 은 두 벡터가 얼마나 유사한지 측정하는 지표가 됨.
- 기존에는 **유클리드 거리(Mean Squared Error, MSE)** 를 이용해서 차이를 직접 비교했다면, 해당 손실 함수는 코사인 유사도와 유사한 방식으로 콘텐츠를 비교.

## 3. 내적을 활용한 Content Loss의 의미

### (1) 내적을 사용하면 방향이 중요해짐
- 기존 MSE 방식에서는 특징 맵 값의 절대적인 차이를 최소화하려고 했었음.

- 내적을 사용하면, 두 벡터가 같은 방향을 가지도록 유도하게 됨.
  
즉, 벡터 크기보다는 특징들이 같은 패턴을 가지도록 만드는 역할을 하게 됨.

### (2) 유사도 기반 콘텐츠 보존 (하지만, 강도는 무시될 수도 있음)
- 벡터의 내적은 일반적으로 코사인 유사도(Cosine Similarity)와 비슷한 성질을 가짐.
- 그렇기에 생성된 이미지가 콘텐츠 이미지와 비슷한 방향을 가지도록 유도되지만, 특징 강도 자체는 보존되지 않을 수도 있음.

## 4. 내적을 사용하는 Content Loss의 장점 & 단점

### 장점
- 더 유연한 콘텐츠 보존 가능
  - 기존 방식보다 좀 더 "느슨하게" 콘텐츠 정보를 보존할 수 있음.
  - 예를 들어, 완벽하게 똑같은 값이 아니어도 방향이 맞으면 유사하다고 판단할 수 있음.
- 명암 변화에 덜 민감
  - 기존 MSE 방식은 픽셀 강도의 절대적인 차이에 민감하지만, 내적을 사용하면 상대적인 구조만 유지할 수 있음.
  - 즉, 밝기 변화나 강도 차이에 덜 민감할 가능성이 있음.
### 단점
- 콘텐츠 구조 유지가 덜 정확할 수 있음
  - 내적은 특징 맵이 비슷한 패턴을 가지도록 유도하지만, 정확한 픽셀 값까지는 일치시키지 않음.
  - 그래서 콘텐츠가 약간 왜곡될 가능성이 있음.
    
- 벡터 크기(강도) 정보가 무시될 수 있음
  - 만약 콘텐츠 이미지의 특징 강도가 크고, 생성된 이미지의 특징 강도가 작다면, 내적 방식에서는 두 이미지가 여전히 유사하다고 판단할 수 있음.
  - 즉, "특징이 있는지 여부"는 유지되지만, "특징이 얼마나 강한지"는 잘 반영되지 않을 수 있음.
## 5. 정리

- 변경시 기존 방식(MSE)보다 덜 정확할 수도 있음.
- 좀 더 유연하게 콘텐츠를 보존하고 싶다면 시도해볼 만한 방법
- 하지만 강도 정보가 손실될 수 있어, 콘텐츠 형태를 유지하는 데 불안정할 수도 있음.

즉, 기존의 MSE 방식은 **"정확한 픽셀 수준 콘텐츠 유지"** 를 목표로 하고,
내적 방식은 **"유사한 패턴을 유지하되 더 유연한 형태"** 로 콘텐츠를 보존하는 느낌.
